---
title: "Surrogate Variable Detecting by permuting columns"
author: "Meilei Jiang"
date: "September 4, 2015"
output: html_document
---


## Review of Surrogate Variable Analysis on mean heterogeneity data

In the previous document, we apply Surrogate Variable Analysis on the mean heterogeneity data. In our toy examples, there are 80 samples. The samples belong to two classes. First 40 samples belong to Class 1 and others belong to Class 2. The samples are collected from two batches: Batch1 and Batch2. 

Each sample has 100 features:

1. $X_1$ is the primary variables effects. For Class 1, $X_1 = 4$; for Class 2, $X_1 = -4$.
2. $X_2$ is the batch effects. For Batch 1, $X_2 \sim \mathbb{N}(2, 1)$; for Batch 2, $X_2 \sim \mathbb{N}(-2, 1)$
  + $\pi_1$ of samples in Class 1 from Batch 1 and $\pi_2$ of samples Class 2 from Batch 1. 
  + The other samples from Batch 2. 
  + The propotion of samples from Batch 1 are $p_1 = \frac{\pi_1 + \pi_2}{2}$ 
  + The propotion of samples from Batch 2 are $p_2 = 1 - p_1$.
  
3. $X_3, \dots, X_{100} \sim \mathbb{N}(0, 1)$ are the random noises.

$R$ is the residual matrix gained by regressing the data matrix $X$ from the primary variable (class label vector) $Y$. Since the primary variables effects are constant, $X_1$ in the residual matrix are 0. Then the situation is equivalent to detect the mean heterogeneity from the Gaussian random noise matrix. 

The simulation results indicate that SVA performs poorly on detecting the surrogate variable in our toy examples. The major problem in their algorithm is that they form the new matrix $R^{*}$ by pemuting each row of $R$ independently to remove any structure in the matrix. However, this operation can not indeed break down the Gaussian mixture structure. In fact, in the $R$, $X_3 \dots X_100$ are Gaussian random noises which are spherical symmetric. Then permuting within these rows will not change the structure of matrix. Permuting $X_2$ will also preserve the Gaussian Mixture structure. Therefore, after permuting the matrix within each row independently, the whole matrix still has a Gaussian mixture.
In our toy examples, a more efficent way to detect the surrogate variable is permuting each columns independently. 

## Simulation Study

```{r, echo = FALSE, warning=FALSE, message = FALSE}
# load the data
load("/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/rdata/mean_hetero_example.RData")

```
#### Non Mean Heterogeneity Data
In this case, $\pi_1 = \pi_2 = 1$. It means that all the samples come from the Batch1. Therefore there is no batch effect in the data. This case is set as a baseline for understanding the behavior of eigenvalues in the analysis.

##### SVA analysis
```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# analysis first data set -------------------------------------------------

M <- train4 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train4$pi_1); pi_2 <- unique(train4$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1
# Expression data
Edata = t(M)
colnames(Edata) = paste0("sample", c(1: dim(Edata)[2]))
Mdata = melt(Edata)

title_ng1 = paste("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ng1 = ggplot(Mdata, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ng1) +
  geom_tile() + 
  scale_fill_gradient2()
print(ng1)
# primary variable of interest
Y <- train4 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod = model.matrix(~ y, data = train4)
## null model
mod0 = model.matrix(~ 1, data = train4)


# estimate the number of latent factors that need to be estimated ---------
n.sv = num.sv(Edata,mod,method="be", B = 1000)
print(paste0("The number of surrogate variable: ", n.sv))

if(n.sv == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj = sva(t(M),mod, mod0,n.sv= 1, B = 5)
} else{
  svobj = sva(t(M),mod, mod0,n.sv= n.sv, B = 5)
}

# Remove the batch effects
fsvobj = fsva(Edata, mod, svobj, Edata)

Edb = fsvobj$db

# visualize the result
trainSV <- data.frame(Y, t(Edb), batch = train4$batch)

ng2 = ggplot(data = trainSV, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() + 
  ylim(-5, 5)

ng3 = ggplot(data = train4, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  ylim(-5, 5)

ng4 =ggplot(data = trainSV, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train4, aes(x = X2, group = batch, col = batch)) 

Mdb = melt(Edb)
title_ng5 = paste("Simulation Data After Removing Surrogate Variables Effects \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ng5 = ggplot(Mdb, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ng5) +
  geom_tile() + 
  scale_fill_gradient2()

print(ng4)
print(ng5)

```

##### PCA behavior
```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

HatB = Edata %*% mod %*% solve(t(mod) %*% mod) 
R = Edata - HatB %*% t(mod)

R.pc = getPcaResult(R, varNames = colnames(R), scale=F, center = F)

R.pv =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc$varDf), value = R.pc$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc$varDf)[1]; n2 = dim(R)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R
for(k in 1:B){
  # make permutation of each row independently
  newE = apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod %*% solve(t(mod) %*% mod) %*% t(mod)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat = melt(pvMat)
pv_stat = EpvMat %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ggplot(data = pv_stat, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red") +
  ylim(0, 0.08)

ggplot(data = EpvMat, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red", size = 2) +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv = as.numeric(unlist(train4 %>% select(batch)))
y = as.numeric(unlist(Y))

R.dir = R.pc$dirDf
R.cor = data.frame(PC = colnames(R.dir), angle = acos(cor(R.dir, y))/pi * 180 )


ggplot(data = R.cor, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj$sv, y))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", 
       title = "Angle between PCs and Primary Variable Effect \n blue dashed line: angle(SV, Primary Variable Effect)") + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

```


#### Balanced Mean Heterogeneity Data

In this case, $\pi_1 = \pi_2 = 0.5$. It indicates the balance in the following two ways:

- The propotion of samples from Batch 1 are the same in the two classes. This also means that the batch vector is orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are the same.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

M <- train0 %>% select(-y, -batch, -pi_1, -pi_2 )
pi_1 <- unique(train0$pi_1); pi_2 <- unique(train0$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1
# Expression data
Edata = t(M)
colnames(Edata) = paste0("sample", c(1: dim(Edata)[2]))
Mdata = melt(Edata)

title_bg1 <- paste("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

bg1 = ggplot(Mdata, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_bg1) +
  geom_tile() + 
  scale_fill_gradient2()

print(bg1)

# primary variable of interest
Y <- train0 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod = model.matrix(~ y, data = train0)
## null model
mod0 = model.matrix(~ 1, data = train0)


# estimate the number of latent factors that need to be estimated ---------
n.sv = num.sv(Edata,mod,method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv))

if(n.sv == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj = sva(t(M),mod, mod0,n.sv= 1, B = 5)
} else{
  svobj = sva(t(M),mod, mod0,n.sv= n.sv, B = 5)
}


# Remove the batch effects
fsvobj = fsva(Edata, mod, svobj, Edata)

Edb = fsvobj$db

# visualize the result
trainSV <- data.frame(Y, t(Edb), batch = train0$batch)

bg2 = ggplot(data = trainSV, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() + 
  ylim(-5, 5)

bg3 = ggplot(data = train0, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() + 
  ylim(-5, 5)

bg4 = ggplot(data = trainSV, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train0, aes(x = X2, group = batch, col = batch))

grid.arrange(bg2, bg3, nrow = 1, ncol = 2)
print(bg4)

Mdb = melt(Edb)
title_bg5 = paste("Simulation Data After Removing Surrogate Variables Effects \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
bg5 = ggplot(Mdb, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_bg5) +
  geom_tile() + 
  scale_fill_gradient2()
print(bg5)


```

##### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# estimate the coefficient of basis matrix
HatB = Edata %*% mod %*% solve(t(mod) %*% mod) 
R = Edata - HatB %*% t(mod)

R.pc = getPcaResult(R, varNames = colnames(R), scale=F, center = F)

R.pv =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc$varDf), value = R.pc$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc$varDf)[1]; n2 = dim(R)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R
for(k in 1:B){
  # make permutation of each row independently
  newE = apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod %*% solve(t(mod) %*% mod) %*% t(mod)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat = melt(pvMat)
pv_stat = EpvMat %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ggplot(data = pv_stat, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red") +
  ylim(0, 0.08)

ggplot(data = EpvMat, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red", size = 2) +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv = as.numeric(unlist(train0 %>% select(batch)))
y = as.numeric(unlist(Y))

R.dir = R.pc$dirDf
R.cor = data.frame(PC = colnames(R.dir), angle = acos(cor(R.dir, bv))/pi * 180 )


ggplot(data = R.cor, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj$sv, bv))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", 
       title = "Angle between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect)") + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

```

#### Unbalanced Mean Heterogeneity Data

In order to better understand the behavior of SVA against heterogeneity, we design three cases for unbalanced batch effects.

##### Case1: $\pi_1 = \pi_2, \pi_1 + \pi_2 \neq 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are the same in the two classes. This also means that the batch vector is orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are different.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 = \pi_2, \pi_1 + \pi_2 < 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.4, \pi_2 = 0.4$. 
- $\pi_1 = 0.1, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train1.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train1.1$pi_1); pi_2 <- unique(train1.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train1.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train1.2$pi_1); pi_2 <- unique(train1.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train1.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train1.1)
## null model
mod10 = model.matrix(~ 1, data = train1.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train1.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() + 
  labs(title = title_ubg2.1) + 
  ylim(-5, 5)

ubg3.1 = ggplot(data = train1.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1) + 
  ylim(-5, 5)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train1.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train1.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train1.2)
## null model
mod20 = model.matrix(~ 1, data = train1.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train1.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2) + 
  ylim(-5, 5)

ubg3.2 = ggplot(data = train1.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2) + 
  ylim(-5, 5)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train1.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1) +
  ylim(0, 0.08)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1) +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train1.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train1.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```

##### Case2: $\pi_1 \neq \pi_2, \pi_1 + \pi_2 = 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are diffrent in the two classes. This also means that the batch vector is not orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are the same.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 > \pi_2, \pi_1 + \pi_2 = 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.6, \pi_2 = 0.4$. 
- $\pi_1 = 0.9, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train2.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train2.1$pi_1); pi_2 <- unique(train2.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train2.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train2.2$pi_1); pi_2 <- unique(train2.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene",  fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train2.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train2.1)
## null model
mod10 = model.matrix(~ 1, data = train2.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train2.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.1) + 
  ylim(-5, 5)

ubg3.1 = ggplot(data = train2.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1) + 
  ylim(-5, 5)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train2.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train2.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train2.2)
## null model
mod20 = model.matrix(~ 1, data = train2.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train2.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2) + 
  ylim(-5, 5)

ubg3.2 = ggplot(data = train2.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2) + 
  ylim(-5, 5)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train2.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1, y = "Proportion of Variance") +
  ylim(0, 0.08)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1, y = "Proportion of Variance") +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train2.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train2.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```

##### Case3: $\pi_1 \neq \pi_2, \pi_1 + \pi_2 \neq 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are different in the two classes. This also means that the batch vector is not orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are different.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 > \pi_2, \pi_1 + \pi_2 < 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.5, \pi_2 = 0.4$. 
- $\pi_1 = 0.4, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train3.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train3.1$pi_1); pi_2 <- unique(train3.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train3.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train3.2$pi_1); pi_2 <- unique(train3.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene",  fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train3.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train3.1)
## null model
mod10 = model.matrix(~ 1, data = train3.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train3.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.1) + 
  ylim(-5, 5)

ubg3.1 = ggplot(data = train3.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1) + 
  ylim(-5, 5)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train3.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train3.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train3.2)
## null model
mod20 = model.matrix(~ 1, data = train3.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv2))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train3.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2) + 
  ylim(-5, 5)

ubg3.2 = ggplot(data = train3.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2) + 
  ylim(-5, 5)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train3.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1, y = "Proportion of Variance") +
  ylim(0, 0.08)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1, y = "Proportion of Variance") +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train3.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- apply(tempR, 2, sample, replace = FALSE)
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2, y = "Proportion of Variance") +
  ylim(0, 0.08)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train3.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```




