---
title: "Surrogate Variable Analysis On Mean Heterogeneity"
author: "Meilei Jiang"
date: "September 2, 2015"
output: html_document
---

## Data variation and heterogeneity

Data variation dues to different kinds of factors:

- modeled factors (primary variables): e.g. class labels
- unmodeled factors : e.g. batch effects:  age, sex
- other variation : e.g. white noise, systematic error

In practice, we want to estimate the effects of primary variables as well as discover the unmodeled factors. For instance, we want to training a model by given class labels. However, the effects of primary variables can be blurred by unmodeled factors. Even the data from the same class can have different distributions or a mixture of distribution, which is also called heterogeneity. 

The surrogate variable analysis (SVA) builds up a linear model for data variation and tries to construct surrogate variables to represent the batch effects.

### Models for data variation

The $p \times n$ random data matrix $X =(x_1, \dots, x_p)'$ contains $p$ features and each feature $x_i$ = $(x_{i1}, \dots, x_{in})'$ has the expression from $n$ samples. The primary variables are $Y = (y_1, \dots, y_n)'$. 

Assume that $\mathbb{E}[x_i|Y] = b_i S(Y) = b_i S$, where $S$ is a matrix of basis function of primary variables with dimension $d \times n$. Also, assume that the marginal model for each $e_i = x_i - b_i S$ is known or approximated sufficiently. In the matrix form, the model can be written as:
                                        $$X = B S + E$$
Moreover, we assume the following decomposition:
                                   $$X = B S + \Gamma G + U$$
Leek and Storey (2008) showed this decomposition exist in the following sense. Suppose that for each $e_i$, there is no Borel measurable function g such that $e_i = g(e_1, \dots, e_{i - 1}, e_{i + 1}, \dots, e_m)$ almost surely. Then there exist matries $\Gamma_{p \times r } G_{r \times n} (r <= n)$ and $U_{p \times n}$ such that:
                                        $$X = B S + \Gamma G + U$$
where the rows of $U$ are jointly independent random vectors so that
$$\mathbb{P}(u_1, u_2, \dots, u_m) = \mathbb{P}(u_1) \times \mathbb{P}(u_2) \times \dots \times \mathbb{P}(u_m)$$
Also $\forall i = 1, \dots, m, u_i \neq 0$ and $u_i = h_i(e_i)$ for a non-random Borel measurable function $h_i$. 


When the structure of $G$ is unknown, a set of surrogate variables $\hat{G}$ are constructed by SVA algorithm.

### Introduction to Surrogate Variable Analysis

#### Detect unmodeled factors 

If there are no batch effects, i.e.$\Gamma = 0$,  the residual matrix $R = X - \hat{B} S$ should not have unusally large variations in some specific directions. To estimate the dimension $r$ of unmodeled factors, the number of unusally large eigenvalues need to be determined. 

In the SVA, an permutation test, a non-parametric approach, is proposed on the residual matrix $R$. It compares the proportion of singular values in $R$ to the proportion of singular values in randomized residual matrices, where each row is permuted individually to break down any structure across rows.

#### Construct surrogate variables

The sva algorithm does not attempt to estimate the unmodeled factors $G$. In fact, the sva constructs surrogate variables to represent the variation not caused by primary variables. 

Say in the previous step, $\hat{K}$ unusally large eigenvalues are identified. For $\lambda_k, k = 1, \cdots, \hat{K}$, take out its corresponding right singular vectors $e_k$. In some sense, $e_k$ can act as surrogate variable $\hat{g}_k$. However, when $\hat{G}$ and $S$ are not orthogonalï¼Œ$\hat{B}$, the estimation of the effects of $S$, is biased. So are $R$ and $e_k$.

To reduce this bias, next, subset of feasures $X^{(r)}$ which have a strong assosiation with $e_k$ are idtentified. Then refit the model with $X^{(r)}$ and obtain the residual matrix $R^{(r)}$. Last, identify the eigenvector $e_{j_k}^{(r)}$ of $R^{(r)}$ which has the largest correlation coefficient with $e_k$.

We can choose $e_{j_k}^{r}$ as $\hat{g}_k$ or continue this iteration. This approach can up-weight the features that show strong assotiation with $G$ and down-weight feasures that show strong association with $S$.

### Apply Surrogate Variable Analysis to Mean Heterogeneity Data

We want to understand whether SVA is robust against mean heterogeneity. Also, we are interested in the distribution of eigenvalues of the heterogeneity data. The SVA is also related to PCA. We want to know how much information about heterogeneity can be captured by PCA as well.

In our simulation, there are 80 samples. The samples belong to two classes. First 40 samples belong to Class 1 and others belong to Class 2. The samples are collected from two batches: Batch1 and Batch2. 

Each sample has 100 features:

- $X_1$ is the primary variables effects. For Class 1, $X_1 = 4$; for Class 2, $X_1 = -4$.
- $X_2$ is the batch effects. For Batch 1, $X_2 \sim \mathbb{N}(2, 1)$; for Batch 2, $X_2 \sim \mathbb{N}(-2, 1)$
  + $\pi_1$ of Class 1 from Batch 1 and $\pi_2$ of Class 2 from Batch 1. 
  + The others from Batch 2. 
  + The propotion of samples from Batch 1 are $p_1 = \frac{\pi_1 + \pi_2}{2}$ 
  + The propotion of samples from Batch 2 are $p_2 = 1 - p_1$.
  
- $X_3, \dots, X_{100} \sim \mathbb{N}(0, 1)$ are the random noises.

Thus, in the simulation, the heterogeneity lies in a 1-dimension space.



```{r, echo = FALSE, warning=FALSE, message = FALSE}
# load the data
load("/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/rdata/mean_hetero_example.RData")

```
#### Non Mean Heterogeneity Data
In this case, $\pi_1 = \pi_2 = 1$. It means that all the samples come from the Batch1. Therefore there is no batch effect in the data. This case is set as a baseline for understanding the behavior of eigenvalues in the analysis.

##### SVA analysis
```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# analysis first data set -------------------------------------------------

M <- train4 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train4$pi_1); pi_2 <- unique(train4$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1
# Expression data
Edata = t(M)
colnames(Edata) = paste0("sample", c(1: dim(Edata)[2]))
Mdata = melt(Edata)

title_ng1 = paste("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ng1 = ggplot(Mdata, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ng1) +
  geom_tile() + 
  scale_fill_gradient2()
print(ng1)
# primary variable of interest
Y <- train4 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod = model.matrix(~ y, data = train4)
## null model
mod0 = model.matrix(~ 1, data = train4)


# estimate the number of latent factors that need to be estimated ---------
n.sv = num.sv(Edata,mod,method="be", B = 1000)
print(paste0("The number of surrogate variable: ", n.sv))

if(n.sv == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj = sva(t(M),mod, mod0,n.sv= 1, B = 5)
} else{
  svobj = sva(t(M),mod, mod0,n.sv= n.sv, B = 5)
}

# Remove the batch effects
fsvobj = fsva(Edata, mod, svobj, Edata)

Edb = fsvobj$db

# visualize the result
trainSV <- data.frame(Y, t(Edb), batch = train4$batch)

ng2 = ggplot(data = trainSV, aes(x = as.factor(batch), y = X2)) + geom_boxplot()

ng3 = ggplot(data = train4, aes(x = as.factor(batch), y = X2)) + geom_boxplot()

ng4 =ggplot(data = trainSV, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train4, aes(x = X2, group = batch, col = batch))

Mdb = melt(Edb)
title_ng5 = paste("Simulation Data After Removing Surrogate Variables Effects \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ng5 = ggplot(Mdb, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ng5) +
  geom_tile() + 
  scale_fill_gradient2()

print(ng4)
print(ng5)

```

##### PCA behavior
```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

HatB = Edata %*% mod %*% solve(t(mod) %*% mod) 
R = Edata - HatB %*% t(mod)

R.pc = getPcaResult(R, varNames = colnames(R), scale=F, center = F)

R.pv =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc$varDf), value = R.pc$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc$varDf)[1]; n2 = dim(R)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R
for(k in 1:B){
  # make permutation of each row independently
  newE = t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod %*% solve(t(mod) %*% mod) %*% t(mod)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat = melt(pvMat)
pv_stat = EpvMat %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ggplot(data = pv_stat, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red")

ggplot(data = EpvMat, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red", size = 2)

# Analysis of the angle between eigenvector and batch vector

bv = as.numeric(unlist(train4 %>% select(batch)))
y = as.numeric(unlist(Y))

R.dir = R.pc$dirDf
R.cor = data.frame(PC = colnames(R.dir), angle = acos(cor(R.dir, y))/pi * 180 )


ggplot(data = R.cor, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj$sv, y))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", 
       title = "Angle between PCs and Primary Variable Effect \n blue dashed line: angle(SV, Primary Variable Effect)") + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

```


#### Balanced Mean Heterogeneity Data

In this case, $\pi_1 = \pi_2 = 0.5$. It indicates the balance in the following two ways:

- The propotion of samples from Batch 1 are the same in the two classes. This also means that the batch vector is orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are the same.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

M <- train0 %>% select(-y, -batch, -pi_1, -pi_2 )
pi_1 <- unique(train0$pi_1); pi_2 <- unique(train0$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1
# Expression data
Edata = t(M)
colnames(Edata) = paste0("sample", c(1: dim(Edata)[2]))
Mdata = melt(Edata)

title_bg1 <- paste("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

bg1 = ggplot(Mdata, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_bg1) +
  geom_tile() + 
  scale_fill_gradient2()

print(bg1)

# primary variable of interest
Y <- train0 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod = model.matrix(~ y, data = train0)
## null model
mod0 = model.matrix(~ 1, data = train0)


# estimate the number of latent factors that need to be estimated ---------
n.sv = num.sv(Edata,mod,method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv))

if(n.sv == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj = sva(t(M),mod, mod0,n.sv= 1, B = 5)
} else{
  svobj = sva(t(M),mod, mod0,n.sv= n.sv, B = 5)
}


# Remove the batch effects
fsvobj = fsva(Edata, mod, svobj, Edata)

Edb = fsvobj$db

# visualize the result
trainSV <- data.frame(Y, t(Edb), batch = train0$batch)

bg2 = ggplot(data = trainSV, aes(x = as.factor(batch), y = X2)) + geom_boxplot()

bg3 = ggplot(data = train0, aes(x = as.factor(batch), y = X2)) + geom_boxplot()

bg4 = ggplot(data = trainSV, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train0, aes(x = X2, group = batch, col = batch))

grid.arrange(bg2, bg3, nrow = 1, ncol = 2)
print(bg4)

Mdb = melt(Edb)
title_bg5 = paste("Simulation Data After Removing Surrogate Variables Effects \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
bg5 = ggplot(Mdb, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_bg5) +
  geom_tile() + 
  scale_fill_gradient2()
print(bg5)


```

##### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# estimate the coefficient of basis matrix
HatB = Edata %*% mod %*% solve(t(mod) %*% mod) 
R = Edata - HatB %*% t(mod)

R.pc = getPcaResult(R, varNames = colnames(R), scale=F, center = F)

R.pv =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc$varDf), value = R.pc$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc$varDf)[1]; n2 = dim(R)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R
for(k in 1:B){
  # make permutation of each row independently
  newE = t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod %*% solve(t(mod) %*% mod) %*% t(mod)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat = melt(pvMat)
pv_stat = EpvMat %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ggplot(data = pv_stat, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red")

ggplot(data = EpvMat, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv, aes(x = Var2, y = value), col = "red", size = 2)

# Analysis of the angle between eigenvector and batch vector

bv = as.numeric(unlist(train0 %>% select(batch)))
y = as.numeric(unlist(Y))

R.dir = R.pc$dirDf
R.cor = data.frame(PC = colnames(R.dir), angle = acos(cor(R.dir, bv))/pi * 180 )


ggplot(data = R.cor, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj$sv, bv))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", 
       title = "Angle between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect)") + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

```
#### Unbalanced Mean Heterogeneity Data

In order to better understand the behavior of SVA against heterogeneity, we design three cases for unbalanced batch effects.

##### Case1: $\pi_1 = \pi_2, \pi_1 + \pi_2 \neq 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are the same in the two classes. This also means that the batch vector is orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are different.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 = \pi_2, \pi_1 + \pi_2 < 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.4, \pi_2 = 0.4$. 
- $\pi_1 = 0.1, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train1.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train1.1$pi_1); pi_2 <- unique(train1.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train1.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train1.2$pi_1); pi_2 <- unique(train1.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
              ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train1.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train1.1)
## null model
mod10 = model.matrix(~ 1, data = train1.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train1.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.1)

ubg3.1 = ggplot(data = train1.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train1.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train1.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train1.2)
## null model
mod20 = model.matrix(~ 1, data = train1.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train1.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2)

ubg3.2 = ggplot(data = train1.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train1.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train1.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train1.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```

##### Case2: $\pi_1 \neq \pi_2, \pi_1 + \pi_2 = 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are diffrent in the two classes. This also means that the batch vector is not orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are the same.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 > \pi_2, \pi_1 + \pi_2 = 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.6, \pi_2 = 0.4$. 
- $\pi_1 = 0.9, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train2.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train2.1$pi_1); pi_2 <- unique(train2.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train2.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train2.2$pi_1); pi_2 <- unique(train2.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene",  fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train2.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train2.1)
## null model
mod10 = model.matrix(~ 1, data = train2.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train2.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.1)

ubg3.1 = ggplot(data = train2.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train2.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train2.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train2.2)
## null model
mod20 = model.matrix(~ 1, data = train2.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train2.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2)

ubg3.2 = ggplot(data = train2.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train2.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train2.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train2.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```

##### Case3: $\pi_1 \neq \pi_2, \pi_1 + \pi_2 \neq 1$

In this case, it indicates:

- The propotion of samples from Batch 1 are different in the two classes. This also means that the batch vector is not orthogonal to primary variable (class indicator vector). 
- The sizes of samples from Batch 1 and Batch 2 are different.

By the symmetric of Batch 1 and Batch 2, we only need to consider the case $\pi_1 > \pi_2, \pi_1 + \pi_2 < 1$. We do simulation for two sets of $\pi_1$ and $\pi_2$:

- $\pi_1 = 0.5, \pi_2 = 0.4$. 
- $\pi_1 = 0.4, \pi_2 = 0.1$.

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

# data processing ---------------------------------------------------------
# first data set
M1 <- train3.1 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train3.1$pi_1); pi_2 <- unique(train3.1$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata1 = t(M1)
colnames(Edata1) = paste0("sample", c(1: dim(Edata1)[2]))
Mdata1 = melt(Edata1)

title_ubg1.1 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.1 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.1 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.1 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.1 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.1 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.1 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.1 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)

ubg1.1 = ggplot(Mdata1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value", title = title_ubg1.1) +
  geom_tile() + 
  scale_fill_gradient2()

# second data set
M2 <- train3.2 %>% select(-y, -batch, -pi_1, -pi_2)
pi_1 <- unique(train3.2$pi_1); pi_2 <- unique(train3.2$pi_2)
p_1 <- (pi_1 + pi_2)/2; p_2 <- 1 - p_1

# Expression data
Edata2 = t(M2)
colnames(Edata2) = paste0("sample", c(1: dim(Edata2)[2]))
Mdata2 = melt(Edata2)

title_ubg1.2 <- paste0("Simulation data \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg2.2 <- paste0("Batch effects after adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg3.2 <- paste0("Batch effects before adjustment \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg4.2 <- paste0("Batch effects of two batches \n pi_1 = ", pi_1, 
                       ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg5.2 = paste0("Simulation data after ajustment \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg6.2 = paste0("Eigenvalues Boxplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg7.2 = paste0("Screeplot \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
title_ubg8.2 = paste0("Angles between PCs and Batch Effect \n blue dashed line: angle(SV, Batch Effect) \n pi_1 = ", pi_1, 
                      ", pi_2 = ", pi_2, "\n p_1 = ", p_1, ", p_2 = ", p_2)
ubg1.2 = ggplot(Mdata2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene",  fill = "Value", title = title_ubg1.2) +
  geom_tile() + 
  scale_fill_gradient2()

grid.arrange(ubg1.1, ubg1.2, nrow = 1, ncol = 2)

# analysis first data set -------------------------------------------------

# primary variable of interest
Y1 <- train3.1 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod1 = model.matrix(~ y, data = train3.1)
## null model
mod10 = model.matrix(~ 1, data = train3.1)


# estimate the number of latent factors that need to be estimated ---------
n.sv1 = num.sv(Edata1, mod1, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv1))

if(n.sv1 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj1 = sva(t(M1),mod1, mod10,n.sv= 1, B = 5)
} else{
  svobj1 = sva(t(M1),mod1, mod10,n.sv= n.sv1, B = 5)
}

# Remove the batch effects
fsvobj1 = fsva(Edata1, mod1, svobj1, Edata1)

Edb1 = fsvobj1$db

# visualize the result
trainSV1 <- data.frame(Y1, t(Edb1), batch = train3.1$batch)

ubg2.1 = ggplot(data = trainSV1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.1)

ubg3.1 = ggplot(data = train3.1, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg3.1)

ubg4.1 = ggplot(data = trainSV1, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train3.1, aes(x = X2, group = batch, col = batch)) + 
  labs(title = title_ubg4.1)

Mdb1 = melt(Edb1)

ubg5.1 = ggplot(Mdb1, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.1) +
  geom_tile() + 
  scale_fill_gradient2()

# analysis second data set -------------------------------------------------

# primary variable of interest
Y2 <- train3.2 %>% select(y)

# make model matrix -------------------------------------------------------
## full model 
mod2 = model.matrix(~ y, data = train3.2)
## null model
mod20 = model.matrix(~ 1, data = train3.2)


# estimate the number of latent factors that need to be estimated ---------
n.sv2 = num.sv(Edata2, mod2, method="be", B = 1000)

print(paste0("The number of surrogate variable: ", n.sv2))

if(n.sv2 == 0){
  print("Mannually setting the number of surrogate variable as 1 to apply the sva algorithm")
  svobj2 = sva(t(M2),mod2, mod20,n.sv= 1, B = 5)
} else{
  svobj2 = sva(t(M2),mod2, mod20,n.sv= n.sv2, B = 5)
}


# Remove the batch effects
fsvobj2 = fsva(Edata2, mod2, svobj2, Edata2)

Edb2 = fsvobj2$db

# visualize the result
trainSV2 <- data.frame(Y2, t(Edb2), batch = train3.2$batch)

ubg2.2 = ggplot(data = trainSV2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot() +
  labs(title = title_ubg2.2)

ubg3.2 = ggplot(data = train3.2, aes(x = as.factor(batch), y = X2)) + 
  geom_boxplot()+
  labs(title = title_ubg3.2)

ubg4.2 = ggplot(data = trainSV2, aes(x = X2, group = batch, col = batch)) + 
  geom_density(linetype = "dashed") + 
  geom_density(data = train3.2, aes(x = X2, group = batch, col = batch)) +
  labs(title = title_ubg4.2)

Mdb2 = melt(Edb2)
ubg5.2 = ggplot(Mdb2, aes(x = Var2, y = Var1, fill = value)) +
  labs(x = "Sample", y = "Gene", fill = "Value",title = title_ubg5.2) +
  geom_tile() + 
  scale_fill_gradient2()

# print the plot
grid.arrange(ubg2.1, ubg3.1, nrow = 1, ncol = 2)
grid.arrange(ubg2.2, ubg3.2, nrow = 1, ncol = 2)
grid.arrange(ubg4.1, ubg4.2, nrow = 1, ncol = 2)
grid.arrange(ubg5.1, ubg5.2, nrow = 1, ncol = 2)
```

###### PCA behavior

```{r, echo = FALSE, warning=FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(sva)
library(reshape2)
library(gridExtra)

source('/Users/meileijiang/researchspace/robust-against-heterogeneity/Surrogate Variable Analysis/pcafuns/R/getPcaResult.R')

# analysis the first data set

# estimate the coefficient of basis matrix
HatB1 = Edata1 %*% mod1 %*% solve(t(mod1) %*% mod1) 
R1 = Edata1 - HatB1 %*% t(mod1)

R.pc1 = getPcaResult(R1, varNames = colnames(R1), scale=F, center = F)

R.pv1 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc1$varDf), value = R.pc1$varDf[,2])

# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc1$varDf)[1]; n2 = dim(R1)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R1
for(k in 1:B){
  # make permutation of each row independently
  newE = t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod1 %*% solve(t(mod1) %*% mod1) %*% t(mod1)
  colnames(newR) = c(1:n1)
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), scale=F, center = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc1$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat1 = melt(pvMat)
pv_stat1 = EpvMat1 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.1 = ggplot(data = pv_stat1, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.1)

ubg7.1 = ggplot(data = EpvMat1, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv1, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv1, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.1)

# Analysis of the angle between eigenvector and batch vector

bv1 = as.numeric(unlist(train3.1 %>% select(batch)))
y1 = as.numeric(unlist(Y1))

R.dir1 = R.pc1$dirDf
R.cor1 = data.frame(PC = colnames(R.dir1), angle = acos(cor(R.dir1, bv1))/pi * 180 )

ubg8.1 = ggplot(data = R.cor1, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj1$sv, bv1))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle",  title = title_ubg8.1) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# analysis second data set -------------------------------------------------

# estimate the coefficient of basis matrix
HatB2 = Edata2 %*% mod2 %*% solve(t(mod2) %*% mod2) 
R2 = Edata2 - HatB2 %*% t(mod2)

R.pc2 = getPcaResult(R2, varNames = colnames(R2), scale=F, center = F)

R.pv2 =  data.frame(Var1 = "Origin", Var2 = rownames(R.pc2$varDf), value = R.pc2$varDf[,2])


# use permutation test to find the unusual large eigenvalues --------------
B = 1000
n1 = dim(R.pc2$varDf)[1]; n2 = dim(R2)[2]
pvMat = matrix(nrow = B, ncol = n1)
tempR = R2
for(k in 1:B){
  # make permutation of each row independently
  newE <- t(apply(tempR, 1, sample, replace = FALSE))
  # refit the model to get the new residual matrix
  newR = newE - newE %*% mod2 %*% solve(t(mod2) %*% mod2) %*% t(mod2)
  colnames(newR) = c(1:n1)  
  # do pca on newR and take out the proprotion variance vector
  newR.pc = getPcaResult(newR, varNames = colnames(newR), center = F, scale = F)
  pvMat[k, 1:n1] = newR.pc$varDf[,2]
  temoR = newR
}

colnames(pvMat) = rownames(R.pc2$varDf)
rownames(pvMat) = paste0("Run", 1:B)

EpvMat2 = melt(pvMat)
pv_stat2 = EpvMat2 %>% 
  group_by(Var2) %>%
  summarise(Median = median(value), Q950 = quantile(value, .95), Q000 = quantile(value, .0)) 

ubg6.2 = ggplot(data = pv_stat2, aes(x = Var2, y = Median) )+ 
  geom_point(col = "blue") +
  geom_errorbar(aes(ymax = Q950, ymin = Q000)) + 
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  labs(title = title_ubg6.2)

ubg7.2 = ggplot(data = EpvMat2, aes(x = Var2, y = value, group = Var1)) + 
  geom_line(col = "blue") +
  geom_line(data = R.pv2, aes(x = Var2, y = value), col = "red") +
  geom_point(data = R.pv2, aes(x = Var2, y = value), col = "red", size = 2) +
  labs(title = title_ubg7.2)

# Analysis of the angle between eigenvector and batch vector

bv2 = as.numeric(unlist(train3.2 %>% select(batch)))
y2 = as.numeric(unlist(Y2))

R.dir2 = R.pc2$dirDf
R.cor2 = data.frame(PC = colnames(R.dir2), angle = acos(cor(R.dir2, bv2))/pi * 180 )

ubg8.2 = ggplot(data = R.cor2, aes(x = PC, y = angle))+ 
  geom_point() + 
  geom_hline(yintercep = acos(cor(svobj2$sv, bv2))/pi * 180, col = "blue", linetype = "dashed") +
  geom_hline(yintercep = 90, col = "red", linetype = "dashed") + 
  labs(x = "PC", y = "Angle", title = title_ubg8.2) + 
  scale_y_continuous(limits = c(0, 180), breaks = seq(0, 180, by = 30))

# print the figures

grid.arrange(ubg6.1, ubg6.2, nrow = 1, ncol = 2)
grid.arrange(ubg7.1, ubg7.2, nrow = 1, ncol = 2)
grid.arrange(ubg8.1, ubg8.2, nrow = 1, ncol = 2)

```